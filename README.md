# RAG QA â€” Grounded Question Answering over Documents

Production-minded Retrieval-Augmented Generation (RAG) system for asking questions over internal documents without hallucinations.

The system retrieves relevant document fragments, decides whether there is enough context, and only then generates an answer.
If the answer is not grounded in documents, it explicitly replies â€œI donâ€™t know based on the provided documents.â€

â¸»

## What does this solve?
- ğŸ” Search and QA over company documents (PDF / TXT / MD)
- ğŸš« No hallucinations â€” strict grounding in sources
- ğŸ§  Clear separation of responsibilities:
  - retrieval
  - decision logic
  - generation
- ğŸ“Š Built-in evaluation and metrics
- ğŸ§ª Tests included

**Typical use cases:**
- Internal knowledge base
- Engineering / standards documentation
- Compliance & procedures
- Corporate policies

Typical use cases:
- Internal knowledge base
- Engineering / standards documentation
- Compliance & procedures
- Corporate policies

â¸»

## High-level pipeline

- documents
- ingest â†’ chunking â†’ embeddings â†’ FAISS index
- retriever â†’ decider â†’ prompt builder â†’ LLM
- answer + sources (or refusal)

Key principle:

LLM does NOT search.
FAISS searches.
LLM only writes answers from retrieved context.

â¸»

## Project structure

app/
â”œâ”€ rag/
â”‚   â”œâ”€ ingest.py        # load documents
â”‚   â”œâ”€ chunking.py      # text â†’ chunks
â”‚   â”œâ”€ embedder.py      # chunks â†’ vectors
â”‚   â”œâ”€ store_faiss.py   # FAISS index
â”‚   â”œâ”€ retriever.py     # vector search + threshold
â”‚   â”œâ”€ decider.py       # should answer or refuse
â”‚   â”œâ”€ prompt.py        # strict prompt builder
â”‚   â”œâ”€ generator.py    # LLM call (JSON output)
â”‚   â”œâ”€ pipeline.py     # full RAG orchestration
â”‚   â””â”€ run_pipeline.py # CLI entry
â”‚
â”œâ”€ utils/               # logging, helpers
â”œâ”€ config.py            # env-based config
â”œâ”€ main.py              # FastAPI app
â””â”€ schemas.py           # API contracts

data/
â”œâ”€ docs/                # your documents
â””â”€ artifacts/index/     # FAISS index + chunks

eval/
â”œâ”€ cases.jsonl          # evaluation cases
â””â”€ run_eval.py          # eval runner

tests/
â”œâ”€ test_retriever.py
â””â”€ test_api.py


â¸»

## Installation

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

Create .env from example:

cp .env.example .env

Set your API key and models inside .env.

â¸»

**Add documents**

Put your files into:

data/docs/

Supported formats:
- .pdf
- .txt
- .md

â¸»

**Build index**

python -m app.rag.ingest

Artifacts created:
- faiss.index
- chunks.jsonl
- stats.json

â¸»

**Ask questions (CLI)**

python -m app.rag.run_pipeline "drawing title block requirements"

Example output:

{
  "answer": "...",
  "sources": [
    {
      "doc_id": "DOC-001",
      "chunk_id": 0,
      "page": 18,
      "score": 0.57
    }
  ]
}


â¸»

**API usage**

Run server:

uvicorn app.main:app --port 8001

POST /ask

{
  "question": "What are title block requirements?"
}

Responses:
- 200 OK â€” answer or explicit refusal
- 422 â€” invalid input
- 500 â€” infrastructure failure only

Swagger available at /docs.

â¸»

### Evaluation

Run RAG evaluation:

python -m eval.run_eval

Metrics:
- hit@k â€” correct document retrieved
- grounded_rate â€” sources present when expected
- refusal_quality â€” correct â€œI donâ€™t knowâ€
- latency & cost

Example summary:

hit@k: 0.93
grounded_rate: 0.93
refusal_quality: 1.00


â¸»

### Design decisions
- FAISS + cosine similarity
- Threshold-based retrieval
- Explicit decider before generation
- Strict JSON-only LLM output
- No re-ranking / no hybrid search (yet)

â¸»

### Limitations & future work
- No BM25 / hybrid retrieval
- No cross-encoder re-ranking
- No layout-aware PDF understanding
- No OCR

Planned:
- re-ranking
- hybrid search
- RAG v2 with multi-step reasoning

â¸»

## Why this repo matters

**This is not a demo toy.**

This is a clean, inspectable, production-grade RAG baseline that:
- refuses when uncertain
- exposes metrics
- is testable
- is extendable