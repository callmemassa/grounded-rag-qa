# RAG QA ‚Äî Grounded Question Answering over Documents

Production-minded Retrieval-Augmented Generation (RAG) system for asking questions over internal documents without hallucinations.

The system retrieves relevant document fragments, decides whether there is enough context, and only then generates an answer.
If the answer is not grounded in documents, it explicitly replies ‚ÄúI don‚Äôt know based on the provided documents.‚Äù

‚∏ª

## What does this solve?
- üîç Search and QA over company documents (PDF / TXT / MD)
- üö´ No hallucinations ‚Äî strict grounding in sources
- üß† Clear separation of responsibilities:
  - retrieval
  - decision logic
  - generation
- üìä Built-in evaluation and metrics
- üß™ Tests included

**Typical use cases:**
- Internal knowledge base
- Engineering / standards documentation
- Compliance & procedures
- Corporate policies

Typical use cases:
- Internal knowledge base
- Engineering / standards documentation
- Compliance & procedures
- Corporate policies

‚∏ª

## High-level pipeline

- documents
- ingest ‚Üí chunking ‚Üí embeddings ‚Üí FAISS index
- retriever ‚Üí decider ‚Üí prompt builder ‚Üí LLM
- answer + sources (or refusal)

Key principle:

LLM does NOT search.
FAISS searches.
LLM only writes answers from retrieved context.

‚∏ª

## Project structure

```text
app/
  rag/
    ingest.py         # load documents
    chunking.py       # text -> chunks
    embedder.py       # chunks -> vectors
    store_faiss.py    # FAISS index
    retriever.py      # vector search + threshold
    decider.py        # should answer or refuse
    prompt.py         # strict prompt builder
    generator.py      # LLM call (JSON output)
    pipeline.py       # full RAG orchestration
    run_pipeline.py   # CLI entry

  utils/              # logging, helpers
  config.py           # env-based config
  main.py             # FastAPI app
  schemas.py          # API contracts

data/
  docs/               # your documents
  artifacts/
    index/            # FAISS index + chunks.jsonl + stats.json

eval/
  cases.jsonl         # evaluation cases
  run_eval.py         # eval runner

tests/
  test_retriever.py
  test_api.py
```

‚∏ª

## Installation

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

Create .env from example:

cp .env.example .env

Set your API key and models inside .env.

‚∏ª

**Add documents**

Put your files into:

data/docs/

Supported formats:
- .pdf
- .txt
- .md

‚∏ª

**Build index**

python -m app.rag.ingest

Artifacts created:
- faiss.index
- chunks.jsonl
- stats.json

‚∏ª

**Ask questions (CLI)**

python -m app.rag.run_pipeline "drawing title block requirements"

Example output:

{
  "answer": "...",
  "sources": [
    {
      "doc_id": "DOC-001",
      "chunk_id": 0,
      "page": 18,
      "score": 0.57
    }
  ]
}


‚∏ª

**API usage**

Run server:

uvicorn app.main:app --port 8001

POST /ask

{
  "question": "What are title block requirements?"
}

Responses:
- 200 OK ‚Äî answer or explicit refusal
- 422 ‚Äî invalid input
- 500 ‚Äî infrastructure failure only

Swagger available at /docs.

‚∏ª

### Evaluation

Run RAG evaluation:

python -m eval.run_eval

Metrics:
- hit@k ‚Äî correct document retrieved
- grounded_rate ‚Äî sources present when expected
- refusal_quality ‚Äî correct ‚ÄúI don‚Äôt know‚Äù
- latency & cost

Example summary:

hit@k: 0.93
grounded_rate: 0.93
refusal_quality: 1.00


‚∏ª

### Design decisions
- FAISS + cosine similarity
- Threshold-based retrieval
- Explicit decider before generation
- Strict JSON-only LLM output
- No re-ranking / no hybrid search (yet)

‚∏ª

### Limitations & future work
- No BM25 / hybrid retrieval
- No cross-encoder re-ranking
- No layout-aware PDF understanding
- No OCR

Planned:
- re-ranking
- hybrid search
- RAG v2 with multi-step reasoning

‚∏ª

## Why this repo matters

**This is not a demo toy.**

This is a clean, inspectable, production-grade RAG baseline that:
- refuses when uncertain
- exposes metrics
- is testable
- is extendable
